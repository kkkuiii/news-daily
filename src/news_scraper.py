import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import time
import logging
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
import sys
from urllib.parse import urljoin, urlparse
import re

# DeepSeek é›†æˆ
from openai import OpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é‚®ä»¶é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
EMAIL_CONFIG = {
    'smtp_server': os.getenv('SMTP_SERVER', 'smtp.qq.com'),
    'smtp_port': int(os.getenv('SMTP_PORT', '587')),
    'sender_email': os.getenv('SENDER_EMAIL', ''),
    'sender_password': os.getenv('SENDER_PASSWORD', ''),
    'receiver_email': os.getenv('RECEIVER_EMAIL', ''),
}

class EnhancedNewsScraper:
    def __init__(self, deepseek_api_key: str = None):
        # å®Œæ•´çš„æ–°é—»æºé…ç½® - å›½å†…å¤–ç»“åˆ
        self.rss_sources = [
            # å›½å†…ç§‘æŠ€æ–°é—»æº
            'https://36kr.com/feed',
            'https://www.pingwest.com/feed',
            'https://www.jiqizhixin.com/rss',
            'https://www.leiphone.com/rss',
            'https://tech.sina.com.cn/rss/index.shtml',
            'https://tech.qq.com/web/rss_xml.htm',
            'https://www.geekpark.net/rss',
            
            # å›½å¤–ç§‘æŠ€æ–°é—»æº
            'https://www.wired.com/feed/rss',
            'https://techcrunch.com/feed/',
            'https://www.theverge.com/rss/index.xml',
            'https://arstechnica.com/feed/',
            'https://www.engadget.com/rss.xml',
        ]
        
        # å®Œæ•´çš„åˆ†ç±»é…ç½®ï¼ˆä¿æŒåŸæ ·ï¼‰
        self.categories = {
            'ç§‘æŠ€': ['technology', 'tech', 'ç§‘æŠ€', 'æ•°ç ', 'äº’è”ç½‘', 'è½¯ä»¶', 'ç¡¬ä»¶', 'åˆ›æ–°', 'startup', 'digital', 'internet'],
            'é‡‘è': ['finance', 'financial', 'é‡‘è', 'è‚¡å¸‚', 'é“¶è¡Œ', 'æŠ•èµ„', 'ç»æµ', 'market', 'economy', 'stock', 'banking'],
            'AI': ['ai', 'artificial intelligence', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç®—æ³•', 'ml', 'neural', 'chatgpt', 'å¤§æ¨¡å‹'],
            'æ•™è‚²': ['education', 'educational', 'æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'å­¦ä¹ ', 'åŸ¹è®­', 'edu', 'university', 'school'],
            'åŒ»ç–—': ['health', 'medical', 'åŒ»ç–—', 'å¥åº·', 'åŒ»é™¢', 'ç–¾ç—…', 'è¯ç‰©', 'medicine', 'hospital', 'doctor'],
            'ç¯ä¿': ['environment', 'climate', 'ç¯ä¿', 'ç¯å¢ƒ', 'æ°”å€™å˜åŒ–', 'å¯æŒç»­', 'green', 'sustainability', 'carbon', 'eco'],
            'æ±½è½¦': ['car', 'auto', 'æ±½è½¦', 'ç”µåŠ¨è½¦', 'ç”µåŠ¨æ±½è½¦', 'tesla', 'ev', 'vehicle'],
            'æ¸¸æˆ': ['game', 'gaming', 'æ¸¸æˆ', 'ç”µç«', 'ç”µå­ç«æŠ€', 'playstation', 'xbox'],
            'åŒºå—é“¾': ['blockchain', 'crypto', 'åŒºå—é“¾', 'åŠ å¯†è´§å¸', 'æ¯”ç‰¹å¸', 'ethereum', 'nft'],
        }
        
        # å­˜å‚¨ç»“æœ
        self.articles_by_category = {category: [] for category in self.categories.keys()}
        self.processed_urls = set()
        
        # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰
        if not deepseek_api_key:
            logger.error("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾› DeepSeek API Keyï¼")
            raise ValueError("DeepSeek API Key is required")
        
        try:
            self.deepseek_client = OpenAI(
                api_key=deepseek_api_key,
                base_url="https://api.deepseek.com"
            )
            logger.info("âœ… DeepSeek API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ DeepSeek API å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def fetch_news_from_rss(self):
        """ä»RSSæºè·å–æ–°é—»"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        successful_sources = 0
        total_articles = 0
        
        for rss_url in self.rss_sources:
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨å¤„ç†RSSæº: {rss_url}")
                
                # ä½¿ç”¨ feedparser è§£æ RSS
                feed = feedparser.parse(rss_url, request_headers=headers)
                
                if feed.bozo and feed.bozo_exception:
                    logger.warning(f"âš ï¸ RSSæºå¯èƒ½æœ‰é—®é¢˜ {rss_url}: {feed.bozo_exception}")
                
                if not feed.entries:
                    logger.warning(f"âš ï¸ RSSæºæ²¡æœ‰å†…å®¹ {rss_url}")
                    continue
                
                # å¤„ç†æ–‡ç« æ¡ç›®
                entries_processed = 0
                for entry in feed.entries[:6]:  # æ¯ä¸ªæºæœ€å¤šå¤„ç†6ç¯‡æ–‡ç« 
                    try:
                        # æå–æ–‡ç« ä¿¡æ¯
                        title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
                        link = getattr(entry, 'link', '')
                        summary = getattr(entry, 'summary', '')
                        description = getattr(entry, 'description', '')
                        content = getattr(entry, 'content', [{}])
                        
                        # åˆå¹¶å¤šç§å†…å®¹æº
                        content_text = ""
                        if summary:
                            content_text = summary
                        elif description:
                            content_text = description
                        elif content and isinstance(content, list) and len(content) > 0:
                            content_text = str(content[0].get('value', ''))
                        
                        # é™åˆ¶å†…å®¹é•¿åº¦
                        if len(content_text) > 400:
                            content_text = content_text[:400] + "..."
                        
                        # å‘å¸ƒæ—¶é—´
                        pub_date = None
                        if hasattr(entry, 'published'):
                            pub_date = entry.published
                        elif hasattr(entry, 'updated'):
                            pub_date = entry.updated
                        
                        # å»é‡æ£€æŸ¥
                        if link in self.processed_urls:
                            continue
                        self.processed_urls.add(link)
                        
                        if link and title:
                            # åˆ†ç±»æ–‡ç« 
                            is_relevant, categories = self.categorize_article(title, content_text)
                            
                            if is_relevant and categories:
                                article_data = {
                                    'title': title.strip(),
                                    'url': link.strip(),
                                    'summary': content_text.strip() if content_text else "æš‚æ— æ‘˜è¦",
                                    'publish_date': pub_date,
                                    'source': urlparse(rss_url).netloc
                                }
                                
                                # æ·»åŠ åˆ°å¯¹åº”åˆ†ç±»
                                for category in categories:
                                    # é¿å…é‡å¤æ·»åŠ 
                                    if not any(a['url'] == link for a in self.articles_by_category[category]):
                                        self.articles_by_category[category].append(article_data)
                                
                                logger.info(f"ğŸ¯ è·å–ç›¸å…³æ–‡ç« : [{','.join(categories)}] {title[:40]}...")
                                entries_processed += 1
                                total_articles += 1
                                
                    except Exception as e:
                        logger.debug(f"å¤„ç†RSSæ¡ç›®æ—¶å‡ºé”™: {e}")
                        continue
                
                if entries_processed > 0:
                    successful_sources += 1
                    logger.info(f"ğŸ“Š ä» {rss_url} å¤„ç†äº† {entries_processed} ç¯‡ç›¸å…³æ–‡ç« ")
                        
            except Exception as e:
                logger.error(f"âŒ å¤„ç†RSSæºå¤±è´¥ {rss_url}: {e}")
                continue
        
        logger.info(f"ğŸ“ˆ æ€»ç»“: å¤„ç†äº† {successful_sources} ä¸ªRSSæºï¼Œå…±è·å– {total_articles} ç¯‡ç›¸å…³æ–‡ç« ")
    
    def categorize_article(self, title: str, content: str = "") -> tuple:
        """å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        full_content = (title + " " + content).lower()
        
        relevant_categories = []
        match_scores = {}
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„åŒ¹é…åˆ†æ•°
        for category, keywords in self.categories.items():
            score = 0
            for keyword in keywords:
                keyword_lower = keyword.lower()
                if keyword_lower in full_content:
                    score += 1
                    # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
                    if keyword_lower in title.lower():
                        score += 2
            
            match_scores[category] = score
        
        # æ ¹æ®åˆ†æ•°ç¡®å®šç›¸å…³åˆ†ç±»ï¼ˆé™ä½é˜ˆå€¼æé«˜å¬å›ç‡ï¼‰
        for category, score in match_scores.items():
            # æ›´å®½æ¾çš„é˜ˆå€¼
            threshold = 1  # é™ä½é˜ˆå€¼
            if score >= threshold:
                relevant_categories.append(category)
        
        return len(relevant_categories) > 0, relevant_categories
    
    def remove_duplicates(self):
        """å»é™¤é‡å¤æ–‡ç« """
        total_before = sum(len(articles) for articles in self.articles_by_category.values())
        
        for category in self.articles_by_category:
            seen_urls = set()
            unique_articles = []
            
            for article in self.articles_by_category[category]:
                if article['url'] not in seen_urls:
                    seen_urls.add(article['url'])
                    unique_articles.append(article)
            
            self.articles_by_category[category] = unique_articles
        
        total_after = sum(len(articles) for articles in self.articles_by_category.values())
        logger.info(f"ğŸ—‘ï¸ å»é‡å®Œæˆ: {total_before} â†’ {total_after} ç¯‡æ–‡ç« ")
    
    def scrape_news(self):
        """æŠ“å–æ–°é—»"""
        logger.info("ğŸŒ å¼€å§‹ä»RSSæºè·å–æ–°é—»...")
        self.fetch_news_from_rss()
        
        # å»é‡
        self.remove_duplicates()
        
        logger.info("ğŸ“ˆ æ–°é—»è·å–å®Œæˆ")
    
    def generate_daily_summary(self) -> str:
        """ç”Ÿæˆä»Šæ—¥æ–°é—»æ‘˜è¦ï¼ˆå¼ºåˆ¶ä½¿ç”¨DeepSeekå¤§æ¨¡å‹ï¼‰"""
        logger.info("ğŸ¤– å¼ºåˆ¶ä½¿ç”¨ DeepSeek å¤§æ¨¡å‹ç”Ÿæˆæ™ºèƒ½æ‘˜è¦...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« æ ‡é¢˜
        all_titles = []
        category_stats = {}
        
        for category, articles in self.articles_by_category.items():
            if articles:
                category_stats[category] = len(articles)
                for article in articles[:10]:  # æ¯ä¸ªåˆ†ç±»æœ€å¤šå–10ç¯‡æ–‡ç« 
                    all_titles.append(f"[{category}] {article['title']}")
        
        if not all_titles:
            error_msg = "ä»Šæ—¥å¯¼è§ˆæ‘˜è¦ï¼šDeepSeek API è°ƒç”¨å¤±è´¥ï¼Œæš‚æ— æ–°é—»å†…å®¹å¯ä¾›åˆ†æã€‚"
            logger.error("âŒ æ²¡æœ‰æ–‡ç« å¯ä¾›åˆ†æ")
            return error_msg
        
        # å‡†å¤‡æ•°æ®
        titles_text = "\n".join(all_titles[:60])  # æœ€å¤š60ç¯‡æ–‡ç« 
        stats_text = ", ".join([f"{cat}:{count}ç¯‡" for cat, count in category_stats.items()])
        
        prompt = f"""
        è¯·åŸºäºä»¥ä¸‹ä»Šæ—¥æ–°é—»æ ‡é¢˜ï¼Œç”Ÿæˆä¸€æ®µ400-600å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚è¦æ±‚ï¼š
        1. ä¸è¦æœºæ¢°å¤è¿°æ ‡é¢˜ï¼Œè¦è¿›è¡Œæ¦‚æ‹¬ä¸ä¸²è”
        2. æç‚¼å‡ºå½“æ—¥æ–°é—»çš„ä¸»è¦è¶‹åŠ¿ã€å…³æ³¨ç„¦ç‚¹æˆ–èˆ†è®ºåŠ¨å‘
        3. é£æ ¼è‡ªç„¶æµç•…ï¼Œå…·æœ‰æ•´ä½“æ„Ÿ
        4. çªå‡ºæœ€é‡è¦çš„å‡ ä¸ªä¸»é¢˜æ–¹å‘
        5. å¯ä»¥é€‚å½“åˆ†æå„é¢†åŸŸçš„å‘å±•æ€åŠ¿
        6. ä»¥ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€è¡¨è¾¾
        
        ä»Šæ—¥æ–°é—»ç»Ÿè®¡ï¼š{stats_text}
        
        æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š
        {titles_text}
        
        è¯·ä»¥"ä»Šæ—¥å¯¼è§ˆæ‘˜è¦ï¼š"å¼€å¤´ï¼Œç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
        """
        
        logger.info(f"ğŸ“Š å‡†å¤‡å‘é€ {len(all_titles)} ç¯‡æ–‡ç« æ ‡é¢˜ç»™ DeepSeek AI")
        
        try:
            response = self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆå’Œç§‘æŠ€è§‚å¯Ÿè€…ï¼Œæ“…é•¿æ€»ç»“å’Œåˆ†ææ–°é—»è¶‹åŠ¿ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                stream=False,
                temperature=0.7,
                max_tokens=1000
            )
            
            summary = response.choices[0].message.content.strip()
            if not summary.startswith("ä»Šæ—¥å¯¼è§ˆæ‘˜è¦ï¼š"):
                summary = "ä»Šæ—¥å¯¼è§ˆæ‘˜è¦ï¼š" + summary
            
            logger.info("âœ… DeepSeek å¤§æ¨¡å‹æ‘˜è¦ç”ŸæˆæˆåŠŸï¼")
            return summary
            
        except Exception as e:
            error_msg = f"ä»Šæ—¥å¯¼è§ˆæ‘˜è¦ï¼šDeepSeek API è°ƒç”¨å¤±è´¥ - {str(e)}ã€‚è¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥ã€‚"
            logger.error(f"âŒ DeepSeek å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return error_msg
    
    def generate_html_report(self) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æ—¥æŠ¥"""
        current_date = datetime.now().strftime("%Y-%m-%d")
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆå¼ºåˆ¶ä½¿ç”¨DeepSeekï¼‰
        daily_summary = self.generate_daily_summary()
        
        # æŒ‰æ–‡ç« æ•°é‡æ’åºåˆ†ç±»
        category_article_counts = [(cat, len(articles)) for cat, articles in self.articles_by_category.items() if articles]
        category_article_counts.sort(key=lambda x: x[1], reverse=True)
        
        # HTML æ¨¡æ¿
        html_content = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥æ–°é—»å¯¼è§ˆ - {current_date}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }}
        .container {{
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        header {{
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #eee;
        }}
        h1 {{
            color: #2c3e50;
            margin: 0 0 10px 0;
        }}
        .meta-info {{
            color: #7f8c8d;
            font-size: 14px;
        }}
        .summary {{
            background-color: #fff8e1;
            padding: 25px;
            border-radius: 8px;
            margin-bottom: 30px;
            border-left: 4px solid #ffc107;
        }}
        .summary h2 {{
            margin-top: 0;
            color: #2c3e50;
        }}
        .category {{
            background-color: #f8f9fa;
            padding: 25px;
            border-radius: 8px;
            margin-bottom: 25px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }}
        .category h2 {{
            margin-top: 0;
            color: #2c3e50;
            border-bottom: 2px solid #eee;
            padding-bottom: 15px;
        }}
        .article-item {{
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 6px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
            transition: transform 0.2s ease;
        }}
        .article-item:hover {{
            transform: translateY(-2px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }}
        .article-title {{
            font-size: 17px;
            font-weight: 600;
            margin: 0 0 10px 0;
        }}
        .article-title a {{
            color: #3498db;
            text-decoration: none;
        }}
        .article-title a:hover {{
            text-decoration: underline;
        }}
        .article-meta {{
            font-size: 13px;
            color: #7f8c8d;
            margin: 8px 0;
        }}
        .article-summary {{
            font-size: 14px;
            color: #555;
            margin: 12px 0 0 0;
            line-height: 1.6;
            background-color: #fafafa;
            padding: 12px;
            border-radius: 4px;
            border-left: 3px solid #3498db;
        }}
        .stats {{
            background-color: #e8f4f8;
            padding: 20px;
            border-radius: 8px;
            margin-top: 40px;
        }}
        .stats h2 {{
            margin-top: 0;
            color: #2c3e50;
        }}
        footer {{
            text-align: center;
            margin-top: 40px;
            padding-top: 25px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-size: 13px;
        }}
        @media (max-width: 768px) {{
            .container {{
                padding: 15px;
            }}
            .article-title {{
                font-size: 15px;
            }}
            .article-summary {{
                font-size: 13px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ“° æ¯æ—¥æ–°é—»å¯¼è§ˆ</h1>
            <div class="meta-info">
                ğŸ“… æ—¥æœŸ: {current_date} | ğŸ• ç”Ÿæˆæ—¶é—´: {current_time}
            </div>
        </header>
        
        <div class="summary">
            <h2>ğŸ¯ ä»Šæ—¥å¯¼è§ˆæ‘˜è¦</h2>
            <p>{daily_summary}</p>
        </div>
        
        """
        
        # æ˜¾ç¤ºæ–‡ç« ï¼ˆæŒ‰åˆ†ç±»ï¼‰
        has_content = False
        for category, count in category_article_counts:
            articles = self.articles_by_category[category]
            if articles:
                has_content = True
                html_content += f"""
        <div class="category">
            <h2>ğŸ“‚ {category} ({count}ç¯‡)</h2>
                """
                
                for i, article in enumerate(articles, 1):
                    clean_title = article['title'].strip()
                    if len(clean_title) > 70:
                        clean_title = clean_title[:70] + "..."
                    
                    html_content += f"""
            <div class="article-item">
                <div class="article-title">{i}. <a href="{article['url']}" target="_blank">{clean_title}</a></div>
                """
                    
                    if article.get('publish_date'):
                        # ç®€åŒ–æ—¥æœŸæ˜¾ç¤º
                        pub_date_str = str(article['publish_date'])
                        if len(pub_date_str) > 16:
                            pub_date_str = pub_date_str[:16]
                        html_content += f'<div class="article-meta">ğŸ• å‘å¸ƒæ—¶é—´: {pub_date_str} | ğŸ“° æ¥æº: {article["source"]}</div>\n'
                    else:
                        html_content += f'<div class="article-meta">ğŸ“° æ¥æº: {article["source"]}</div>\n'
                    
                    # æ˜¾ç¤ºæ‘˜è¦ï¼ˆç¡®ä¿æ‘˜è¦ä¸ä¸ºç©ºï¼‰
                    summary_text = article.get('summary', '').strip()
                    if summary_text and summary_text != "æš‚æ— æ‘˜è¦" and len(summary_text) > 10:
                        # æ¸…ç†HTMLæ ‡ç­¾
                        clean_summary = re.sub('<[^<]+?>', '', summary_text)
                        if len(clean_summary) > 200:
                            clean_summary = clean_summary[:200] + "..."
                        html_content += f'<div class="article-summary">ğŸ“ {clean_summary}</div>\n'
                    elif summary_text == "æš‚æ— æ‘˜è¦":
                        html_content += f'<div class="article-summary">ğŸ“ æš‚æ— æ‘˜è¦</div>\n'
                    
                    html_content += '            </div>\n'
                
                html_content += '        </div>\n'
        
        if not has_content:
            html_content += """
        <div class="category">
            <h2>âš ï¸ æš‚æ— ç›¸å…³æ–°é—»</h2>
            <p>å½“å‰æ—¶é—´æ®µå†…æœªæ‰¾åˆ°ç¬¦åˆå…³é”®è¯åˆ†ç±»çš„æ–°é—»ã€‚</p>
        </div>
            """
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(articles) for articles in self.articles_by_category.values())
        html_content += f"""
        <div class="stats">
            <h2>ğŸ“Š è¯¦ç»†ç»Ÿè®¡</h2>
            <p><strong>æ€»æ–‡ç« æ•°:</strong> {total_articles}</p>
            <p><strong>æ¶‰åŠåˆ†ç±»:</strong> {len([cat for cat, count in category_article_counts if count > 0])}</p>
            <p><strong>å¤„ç†æ–°é—»æº:</strong> {len(self.rss_sources)}</p>
            """
        
        for category, count in category_article_counts:
            if count > 0:
                html_content += f"<p>- {category}: {count}ç¯‡</p>\n"
        
        html_content += """
        </div>
        
        <footer>
            <p>ğŸ“Š æ–°é—»æ—¥æŠ¥è‡ªåŠ¨ç”Ÿæˆ | ğŸ• ç”Ÿæˆæ—¶é—´: """ + current_time + """ | ğŸ¤– Powered by DeepSeek AI å¤§æ¨¡å‹</p>
        </footer>
    </div>
</body>
</html>"""
        
        return html_content

class EmailSender:
    """é‚®ä»¶å‘é€å™¨"""
    
    @staticmethod
    def send_html_email(html_content: str, subject: str = None, config: dict = None) -> bool:
        """å‘é€HTMLé‚®ä»¶"""
        if not config:
            config = EMAIL_CONFIG
        
        required_fields = ['sender_email', 'sender_password', 'receiver_email']
        if not all(config.get(field) for field in required_fields):
            logger.error("âŒ é‚®ä»¶é…ç½®ä¸å®Œæ•´")
            return False
        
        if not subject:
            subject = f"æ¯æ—¥æ–°é—»å¯¼è§ˆ-{datetime.now().strftime('%Y-%m-%d')}"
        
        try:
            # åˆ›å»ºé‚®ä»¶å¯¹è±¡
            msg = MIMEMultipart('alternative')
            msg['From'] = config['sender_email']
            msg['To'] = config['receiver_email']
            msg['Subject'] = Header(subject, 'utf-8')
            
            # æ·»åŠ HTMLå†…å®¹
            html_part = MIMEText(html_content, 'html', 'utf-8')
            msg.attach(html_part)
            
            # è¿æ¥SMTPæœåŠ¡å™¨å¹¶å‘é€
            server = smtplib.SMTP(config['smtp_server'], config['smtp_port'])
            server.starttls()
            server.login(config['sender_email'], config['sender_password'])
            server.send_message(msg)
            server.quit()
            
            logger.info("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ–°é—»æ—¥æŠ¥ä»»åŠ¡...")
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
    SENDER_EMAIL = os.getenv('SENDER_EMAIL')
    SENDER_PASSWORD = os.getenv('SENDER_PASSWORD')
    RECEIVER_EMAIL = os.getenv('RECEIVER_EMAIL')
    
    # éªŒè¯å¿…è¦é…ç½®
    if not DEEPSEEK_API_KEY:
        logger.error("âŒ é”™è¯¯ï¼šå¿…é¡»è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼")
        logger.error("è¯·åœ¨ GitHub Secrets ä¸­æ·»åŠ  DEEPSEEK_API_KEY")
        sys.exit(1)
    
    if not all([SENDER_EMAIL, SENDER_PASSWORD, RECEIVER_EMAIL]):
        logger.error("âŒ è¯·è®¾ç½®å®Œæ•´çš„é‚®ä»¶é…ç½®ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # æ›´æ–°é‚®ä»¶é…ç½®
    global EMAIL_CONFIG
    EMAIL_CONFIG.update({
        'sender_email': SENDER_EMAIL,
        'sender_password': SENDER_PASSWORD,
        'receiver_email': RECEIVER_EMAIL,
    })
    
    try:
        # åˆ›å»ºæ–°é—»æŠ“å–å™¨ï¼ˆå¼ºåˆ¶ä½¿ç”¨DeepSeekï¼‰
        logger.info("ğŸ”§ åˆå§‹åŒ– DeepSeek å¤§æ¨¡å‹å®¢æˆ·ç«¯...")
        scraper = EnhancedNewsScraper(deepseek_api_key=DEEPSEEK_API_KEY)
        
        # å¼€å§‹æŠ“å–æ–°é—»
        logger.info("ğŸŒ å¼€å§‹æŠ“å–æ–°é—»...")
        scraper.scrape_news()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(articles) for articles in scraper.articles_by_category.values())
        logger.info(f"ğŸ“Š æŠ“å–ç»Ÿè®¡ - æ€»æ–‡ç« æ•°: {total_articles}")
        for category, articles in scraper.articles_by_category.items():
            if articles:
                logger.info(f"   ğŸ“‚ {category}: {len(articles)}ç¯‡")
        
        # ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¼ºåˆ¶ä½¿ç”¨DeepSeekç”Ÿæˆæ‘˜è¦ï¼‰
        html_content = scraper.generate_html_report()
        logger.info("ğŸ‰ æ–°é—»æ—¥æŠ¥ç”Ÿæˆå®Œæˆ!")
        
        # ä¿å­˜HTMLæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        with open('/tmp/news_report.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info("ğŸ’¾ HTMLæŠ¥å‘Šå·²ä¿å­˜åˆ° /tmp/news_report.html")
        
        # å‘é€é‚®ä»¶
        email_sent = EmailSender.send_html_email(
            html_content=html_content,
            subject=f"æ¯æ—¥æ–°é—»å¯¼è§ˆ-{datetime.now().strftime('%Y-%m-%d')} - DeepSeek AI ç”Ÿæˆ",
            config=EMAIL_CONFIG
        )
        
        if email_sent:
            logger.info("ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            logger.error("ğŸ“§ é‚®ä»¶å‘é€å¤±è´¥ï¼")
            
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
